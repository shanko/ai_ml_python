# -*- coding: utf-8 -*-
"""openai_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VizB4B876yJg81SxGNWIIxbgK2wJ5tOu

## Module 3 - Assignment 1
### In this assignment, you'll learn how to interact with the OpenAI API:
"""

from openai import OpenAI
import os

# from google.colab import userdata
# api_key = userdata.get('OPENAI_API_KEY')
api_key = os.getenv("OPENAI_API_KEY")

# print(api_key)
client = OpenAI(api_key=api_key)

# ---- Basic text generation ----
# To get valid model Ids use this command:
# curl -s "https://api.openai.com/v1/models" -H "Authorization: Bearer $OPENAI_API_KEY"

# Python equivalent of the curl command above
import requests


def get_available_models():
    headers = {"Authorization": f"Bearer {api_key}"}
    response = requests.get("https://api.openai.com/v1/models", headers=headers)
    if response.status_code == 200:
        models = response.json()
        return models
    else:
        print(f"Error: {response.status_code}")
        return None


# Get available models and filter for chat-capable models
available_models = get_available_models()
chat_models = []
non_chat_models = []
if available_models:
    # Filter for models that support chat completions (typically gpt models)
    for model in available_models["data"]:
        model_id = model["id"]
        print(model_id)
        if "gpt" in model_id.lower() and not model_id.startswith("ft:"):
            chat_models.append(model_id)
        else:
            non_chat_models.append(model_id)


# Use the available chat models for completions
print(f"\nFound {len(chat_models)} chat-capable models")
print("Testing chat completions with available models:\n")

for model_id in chat_models:
    try:
        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "user", "content": "Write me a limerick on the name John"}
            ],
            temperature=0.7,
            max_tokens=150,
        )
        print(f"Model: {model_id}")
        print(response.choices[0].message.content)
        print("-" * 50)
    except Exception as e:
        print(f"Error with model {model_id}: {str(e)}")
        print("-" * 50)

# Print all non-chat models
print("\nNon-chat models:")
print(non_chat_models)

# Write code that takes in a for loop and sets different temperatures like 0.0, 0.5, 1.0, etc
# Your code here
# Write code for different models and test with different temperatures for each

"""### Questions:
1) What is the effect of changing the temperature parameter, and why might one choose different values depending on the task?
2) What is the difference between "system", "user", and "assistant" roles in the messages parameter?
3) In what scenarios would you want to use a deterministic model output vs. a creative/randomized output?
4) How would increasing the number of messages in the messages list affect the response of the model?
"""
